%!TEX root = report.tex
In order to successfully implement gradient descent as a learning algorithm, one has to be careful in tuning its parameters. 
If the parameters are chosen incorrectly, the algorithm can quickly overfit (high variance) or underfit (high bias).
For our given dataset we have found that these parameters are most suited:
\begin{align*}
\eta &= 0.01 \\
P &= 4000 \\
t_{max} &= 100
\end{align*}

We have found that these parameters do indeed work correctly, as Figure~\ref{fig:costs_P4000} illustrates. 
The training error approaches a low value quite quickly and the test error approaches a constant value which is reasonably small (\(\sim 0.002\)).
We are quite pleased with these results.