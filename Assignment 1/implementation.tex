%!TEX root = report.tex
The Rosenblatt perceptron algorithm has been implemented in \emph{MATLAB}.
The training phase of the network has been performed with \(N=25\text{, } 50 \text{ and } 100\) for \(\alpha = 0.7, 0.8, 0.9 \dots 3\).
Here \(N\) is the size of the network, expressed in the number of neurons, and \(\alpha\) is the constant of proportionality in the relation \(P = \alpha N\).

\(n_D = 200\) different independent datasets are consecutively presented to the perceptron for training.
The maximimum number of sweeps through the input patterns before we give up on finding a weight vector that seperates the patterns is \(n_{max} = 500\).
This means that for all of the \(n_D\) independent datasets, at most \(n_{max}\) sweeps are performed to determine whether or not the dataset is linearly separable.

The patterns are generated randomly, independent of their labels.
The datasets are generated in one step as a \(P \times N\) matrix of random values from a Gaussian distribution, generated by \emph{MATLAB}'s \texttt{randn} function.
At the end of each row, a random label \(\pm 1\) is appended which is generated using \texttt{randi(2)}.\footnote{
	This returns an integer that is either \(1\) or \(2\).
	These values are mapped to \(-1\) and \(1\) by first multiplying with \(2\), and subsequently subtracting \(3\).
}

For each dataset, the Rosenblatt algorithm is performed as described in the practical assignment.
For each pattern, it is determined whether the weights correctly classify this pattern according to its label.
If it correctly classifies the pattern, the weights are correct for this pattern, so nothing has to change.
If it doesn't correctly classify the pattern, the weights are updated to come closer to the solution.

If during one entire sweep, the weights do not change, this means that the weights correctly classify all input patterns, and thus dataset is linearly separable, and the perceptron has been trained successfully.
Otherwise the algorithm continues to try to separate the dataset until \(n_{max}\) is reached.
When \(n_{max}\) is reached, it is assumed that the dataset is not linearly separable, and the training was unsuccessful.