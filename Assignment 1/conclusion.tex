%!TEX root = report.tex
The Rosenblatt algorithm is shown to be quite successful in training perceptrons.
When increasing the size of the network, the success rate converges towards the step function in Equation~\ref{eq:step}.

This leads us to conclude that the capacity of an \(N\)-dimensional hyperplane (the weight vector \(\vec{w}\)) is \(2N\).
That is, a perceptron with \(N\) input neurons can store \(2N\) independent input patterns.

We have observed the storage capacity $\alpha_c$ as expressed by the number of datasets, $P$, and the number of input neurons of the network, $N$. We found that the number of datasets the network can separate is $P = 1.75 \times N$. This is close value found in the literature, which is $\alpha_c = 2$. This difference is due to the true value of $\alpha_c$ was determined as $N \rightarrow \infty$. Since our number of neurons is limited this value will not be reached.