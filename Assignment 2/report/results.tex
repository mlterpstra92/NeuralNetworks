%!TEX root = report.tex
Running the Minover algorithm on a student perceptron yields a few interesting results.
We present several results we have found here and discuss them in section~\ref{sec:discussion}. 

In all cases the number of neurons was 20, the values of \(\alpha = (0.1, 0.2, 0.3, \ldots 4.9, 5.0)\).
The value of delta was \(10^{-5}\) and the value of \(\text{n}_{\text{max}} = 200\).

\myfigure{
	\includegraphics[width=\columnwidth]{learningcurve_100_innerloops_3.eps}%
	\figcaption{The learning curve of the student perceptron. It shows the generalization error against \(\alpha\). This is the average of 100 trainings per \(\alpha\)-value}
	\label{fig:learningcurve}
}

Figure~\ref{fig:learningcurve} shows the learning curve of the perceptron.
It shows the generalization error against the value of \(\alpha\), which is the number patterns over the number of input neurons. 

The generalization error determines the probability that disagreement occurs between the teacher perceptron and the student perceptron.
Figures~\ref{fig:generalizationerror_50}-\ref{fig:generalizationerror_150} show the generalization error as a function of time.
\myfigure{
	\includegraphics[width=\columnwidth]{generalizationerror_P_50.eps}%
	\figcaption{Generalization error for \(P=50\)}
	\label{fig:generalizationerror_50}
}
\myfigure{
	\includegraphics[width=\columnwidth]{generalizationerror_P_100.eps}%
	\figcaption{Generalization error for \(P=100\)}
	\label{fig:generalizationerror_100}
}
\myfigure{
	\includegraphics[width=\columnwidth]{generalizationerror_P_150.eps}%
	\figcaption{Generalization error for \(P=150\)}
	\label{fig:generalizationerror_150}
}
It can be seen in Figures~\ref{fig:generalizationerror_50}-\ref{fig:generalizationerror_150} that error converges relatively fast towards a certain value.

Moreover, the Minover algorithm has also been run on noisy data.
There was 10\% noise added to the data.
This means that approximately 10\% of the labels $S^{\mu}_R$ had a flipped sign as opposed to what the teacher perceptron trained.
This should show how robust the algorithm is with respect to noise.
\myfigure{
	\includegraphics[width=\columnwidth]{generalizationerror_noisy.eps}%
	\figcaption{Generalization error for \(P=50\) with 10\% noise added.}
	\label{fig:generalizationerror_noisy}
}

Figure~\ref{fig:generalizationerror_noisy} shows the generalization error of the perceptron training with 1\% noisy data.
In comparison with Figure~\ref{fig:generalizationerror_50} it is visible that the system still converges to some value, but the convergence value is higher than with non-noisy training data. 

\myfigure{
	\includegraphics[width=\columnwidth]{learningcurve_noise_001.eps}%
	\figcaption{Learning curve of a student perceptron where 1\% noise is added to the data}
	\label{fig:noisecurve}
}
Figure~\ref{fig:noisecurve} shows the learning curve as figure~\ref{fig:learningcurve} does, but 1\% noise is added.
They are somewhat similar but the curve of Figure~\ref{fig:noisecurve} is less smooth as well as having an overall higher generalized error value.