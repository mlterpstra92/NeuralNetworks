%!TEX root = report.tex
The perceptron was seen in the previous assignment as the basic building block for complex neural networks.
The Rosenblatt algorithm that was discussed allows a perceptron to store input patterns.

The Minover\footnote{Abbreviation for minimal overlap, see \cite{minover}} algorithm, which is discussed in this report, allows a perceptron to learn a rule from a so-called teacher perceptron, where it is not only important that the input patterns are stored, but also that they are stored in the most stable way.
The perceptron that is most stable maximizes the minimal distance between the hyperplane and the input vectors\cite{perceptron_slides2}.
This makes the perceptron less sensitive to noise or small variations in the input patterns.

In this problem, the only available information is the example data:
\[
	\mathbb{D} =
	\left\{
		\pmb{\xi}^\mu, S^\mu_R
	\right\}_{\mu=1}^{P}
\]
where the output labels \(S^\mu_R = \pm 1\) are defined by the teacher perceptron's weight vector \(\mathbf{w}^*\) as
\(
	S^\mu_R =
	\operatorname{sign}(
		\mathbf{w}^* \cdot \pmb{\xi}^\mu
	)
\). 

The stability of an input pattern \(\nu\) is given by:
\[
	\kappa^\nu (t) =
	\frac{
		\mathbf{w}(t) \cdot \pmb{\xi}^\nu S^\nu_R
	}{
		\abs{\mathbf{w}(t)}
	}
	\text{,}
\]
which is, in the geometrical interpretation, the distance between the hyperplane and the input vector\footnote{We try to use the term \textit{input vector} when we're discussing the geometrical interpretation, and the term \textit{input pattern} in general.} \(\nu\).

First of all, the stabilities of all \(\nu\) must be non-negative, because otherwise we have
\(
	\mathbf{w}(t) \cdot \pmb{\xi}^\nu S^\nu_R = 
	S^\nu_H S^\nu_R < 0
\)
, which means that the input pattern is classified incorrectly by the student perceptron.

Secondly, to achieve the most stable classification, of all \(\nu\), the least stable one must be as stable as possible.
In other words, if we let \(\mu(t)\) be the least stable pattern at time \(t\), we want to maximize \(\kappa^{\mu(t)}(t)\).
The Minover algorithm does exactly this.

In other words, the Minover algorithm finds the input vector \(\mu(t)\) with
\[ 
	\kappa^{\mu(t)}(t) = \underset{\nu}{\operatorname{min}} 
	\left\{
		\kappa^\nu (t)
	\right\}
	\text{.}
\]
When this input vector \(\mu(t)\) is found, the stability is increased by moving the hyperplane (which is defined by the weight vector) away from\footnote{
	Actually, not always \textit{away from} the input vector, but only when the \(S_H=S_R\). If \(S_H \neq S_R\), the update step moves the hyperplane \textit{towards} (and maybe even over) the input vector.
} \(\mu(t)\), by using the following Hebbian update step:
\[
	\mathbf{w}(t+1) =
	\mathbf{w}(t) + \frac{1}{N} \pmb{\xi}^{\mu(t)}S^{\mu(t)}_R
	\text{.}
\]

When the stabilities are stabilized, i.e., none of them increase anymore with a value larger than some threshold value, the algorithm terminates.